<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Content-Language" content="en">

    <meta name="author" content="Joachim Wiberg">
    <meta name="description" content="GRE tunnels are useful in many ways.  This blog post shows how to set up
multicast routing with pimd over a
GRE tunnel.  To achieve this we will also set up OSPF over GRE with
Quagga, because PIM, unlike DVMRP (mrouted),
require unicast routing rules to be established.
       .----{ Intranet }----.
      /    192.168.1.0/24    \
     /                        \
.10 /                          \.20
.--&#39;---. .1  GRE Tunnel  .2 .---`--.
|      |====================|      |
|  R1  |   172.16.16.0/30   |  R2  |
|      |                    |      |
&#39;--.---&#39;                    &#39;------&#39;
   | .1                        | .1 
   |    10.0.1.0/24            |    10.0.2.0/24
   | .2                        | .2 
.--&#39;---.                    .--&#39;---.
|      |                    |      |
|  C1  |                    |  C2  |
|      |                    |      |
&#39;------&#39;                    &#39;------&#39;
In this post we are using the home WiFi network, 192.168.1.0/24, to hook
up the GRE tunnel.  It is just as easy to extend this to a big corporate
Intranet with more routers between R1 and R2.  As long as that IT
department takes care of the unicast routing between R1 and R2 so
that the GRE tunnel can be established.">
    <meta name="keywords" content="blog,developer,personal">

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Multicast routing with PIM-SM over GRE"/>
<meta name="twitter:description" content="GRE tunnels are useful in many ways.  This blog post shows how to set up
multicast routing with pimd over a
GRE tunnel.  To achieve this we will also set up OSPF over GRE with
Quagga, because PIM, unlike DVMRP (mrouted),
require unicast routing rules to be established.
       .----{ Intranet }----.
      /    192.168.1.0/24    \
     /                        \
.10 /                          \.20
.--&#39;---. .1  GRE Tunnel  .2 .---`--.
|      |====================|      |
|  R1  |   172.16.16.0/30   |  R2  |
|      |                    |      |
&#39;--.---&#39;                    &#39;------&#39;
   | .1                        | .1 
   |    10.0.1.0/24            |    10.0.2.0/24
   | .2                        | .2 
.--&#39;---.                    .--&#39;---.
|      |                    |      |
|  C1  |                    |  C2  |
|      |                    |      |
&#39;------&#39;                    &#39;------&#39;
In this post we are using the home WiFi network, 192.168.1.0/24, to hook
up the GRE tunnel.  It is just as easy to extend this to a big corporate
Intranet with more routers between R1 and R2.  As long as that IT
department takes care of the unicast routing between R1 and R2 so
that the GRE tunnel can be established."/>

    <meta property="og:title" content="Multicast routing with PIM-SM over GRE" />
<meta property="og:description" content="GRE tunnels are useful in many ways.  This blog post shows how to set up
multicast routing with pimd over a
GRE tunnel.  To achieve this we will also set up OSPF over GRE with
Quagga, because PIM, unlike DVMRP (mrouted),
require unicast routing rules to be established.
       .----{ Intranet }----.
      /    192.168.1.0/24    \
     /                        \
.10 /                          \.20
.--&#39;---. .1  GRE Tunnel  .2 .---`--.
|      |====================|      |
|  R1  |   172.16.16.0/30   |  R2  |
|      |                    |      |
&#39;--.---&#39;                    &#39;------&#39;
   | .1                        | .1 
   |    10.0.1.0/24            |    10.0.2.0/24
   | .2                        | .2 
.--&#39;---.                    .--&#39;---.
|      |                    |      |
|  C1  |                    |  C2  |
|      |                    |      |
&#39;------&#39;                    &#39;------&#39;
In this post we are using the home WiFi network, 192.168.1.0/24, to hook
up the GRE tunnel.  It is just as easy to extend this to a big corporate
Intranet with more routers between R1 and R2.  As long as that IT
department takes care of the unicast routing between R1 and R2 so
that the GRE tunnel can be established." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://troglobit.com/2016/07/05/multicast-routing-with-pim-sm-over-gre/" />
<meta property="article:published_time" content="2018-01-23T20:39:36+00:00" />
<meta property="article:modified_time" content="2018-01-23T20:39:36+00:00" />


    <title>
  Multicast routing with PIM-SM over GRE · The Last Outpost
</title>

    
      <link rel="canonical" href="https://troglobit.com/2016/07/05/multicast-routing-with-pim-sm-over-gre/">
    

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.1.7/css/fork-awesome.min.css" integrity="sha256-gsmEoJAws/Kd3CjuOQzLie5Q3yshhvmo7YNtBG7aaEY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />

    
      
      
      <link rel="stylesheet" href="https://troglobit.com/css/coder.min.9836c03fe5c87d102278a33e86d0591ef36c89b1e17e8e547ebf84c05cee010e.css" integrity="sha256-mDbAP&#43;XIfRAieKM&#43;htBZHvNsibHhfo5Ufr&#43;EwFzuAQ4=" crossorigin="anonymous" media="screen" />
    

    

    
      
        
        
        <link rel="stylesheet" href="https://troglobit.com/css/coder-dark.min.717236c74e0a5208ef73964a9f44c6b443b689a95b270d8b2a40d0c012460dac.css" integrity="sha256-cXI2x04KUgjvc5ZKn0TGtEO2ialbJw2LKkDQwBJGDaw=" crossorigin="anonymous" media="screen" />
      
    

    
      <link rel="stylesheet" href="https://troglobit.com/css/custom.css" />
    

    

    <link rel="icon" type="image/png" href="https://troglobit.com/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="https://troglobit.com/favicon.ico" sizes="16x16">

    <link rel="apple-touch-icon" href="https://troglobit.com/images/apple-touch-icon.png">
    <link rel="apple-touch-icon" sizes="180x180" href="https://troglobit.com/images/apple-touch-icon.png">

    

    <meta name="generator" content="Hugo 0.68.3" />
  </head>

  
  
    
  
  <body class="colorscheme-auto"
        onload=""
  >
    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="https://troglobit.com/">
      The Last Outpost
    </a>
    
      
        <span id="dark-mode-toggle" class="float-right">
          <i class="fa fa-adjust fa-fw" aria-hidden="true"></i>
        </span>
      
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link" href="https://troglobit.com/about/">About</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="https://troglobit.com/post/">Blog</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="https://troglobit.com/howto/">HowTos</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="https://troglobit.com/projects/">Projects</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="https://resume.troglobit.com">Resumé</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="https://ftp.troglobit.com">FTP</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="https://git.troglobit.com">GIT</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="https://deb.troglobit.com">DEB</a>
            </li>
          
        
        
        
          <li class="navigation-item separator">
            <span>|</span>
          </li>
        
      </ul>
    
  </section>
</nav>


      <div class="content">
        
  <section class="container page">
  <article>
    <header>
      <h1>Multicast routing with PIM-SM over GRE</h1>
    </header>

    <p>GRE tunnels are useful in many ways.  This blog post shows how to set up
multicast routing with <a href="https://github.com/troglobit/pimd/">pimd</a> over a
GRE tunnel.  To achieve this we will also set up OSPF over GRE with
<a href="http://www.quagga.net">Quagga</a>, because PIM, unlike DVMRP (<code>mrouted</code>),
require unicast routing rules to be established.</p>
<div class="highlight"><pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">       .----{ Intranet }----.
      /    192.168.1.0/24    \
     /                        \
.10 /                          \.20
.--&#39;---. .1  GRE Tunnel  .2 .---`--.
|      |====================|      |
|  R1  |   172.16.16.0/30   |  R2  |
|      |                    |      |
&#39;--.---&#39;                    &#39;------&#39;
   | .1                        | .1 
   |    10.0.1.0/24            |    10.0.2.0/24
   | .2                        | .2 
.--&#39;---.                    .--&#39;---.
|      |                    |      |
|  C1  |                    |  C2  |
|      |                    |      |
&#39;------&#39;                    &#39;------&#39;
</code></pre></div><p>In this post we are using the home WiFi network, 192.168.1.0/24, to hook
up the GRE tunnel.  It is just as easy to extend this to a big corporate
Intranet with more routers between <code>R1</code> and <code>R2</code>.  As long as that IT
department takes care of the unicast routing between <code>R1</code> and <code>R2</code> so
that the GRE tunnel can be established.</p>
<p>Now, on router R1 we set up the first GRE tunnel endpoint:</p>
<div class="highlight"><pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">ip tunnel add gre0 mode gre remote 192.168.1.20 <span style="color:#24909d">local</span> 192.168.1.10 ttl <span style="color:#3677a9">64</span>
ip link <span style="color:#24909d">set</span> gre0 multicast on
ip link <span style="color:#24909d">set</span> gre0 up
ip addr add 172.16.16.1/30 dev gre0
</code></pre></div><p>We do not add any static route for <code>R1</code> to reach the LAN on <code>R2</code> that
<code>C2</code> is connected to, that is for OSPF to add dynamically for us later.
Notice, hower, that we must explicitly enable the multicast flag on the
GRE interface, it is not enabled by default in Linux.</p>
<p>On router <code>R2</code> we can now set up the other side of the GRE tunnel:</p>
<div class="highlight"><pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">ip tunnel add gre0 mode gre remote 192.168.1.10 <span style="color:#24909d">local</span> 192.168.1.20 ttl <span style="color:#3677a9">64</span>
ip link <span style="color:#24909d">set</span> gre0 multicast on
ip link <span style="color:#24909d">set</span> gre0 up
ip addr add 172.16.16.2/30 dev gre0
</code></pre></div><p>Setup of OSPF in Debian or Ubuntu distributions is only an <code>apt-get</code>
away followed by enabling the zebra and ospf daemons:</p>
<div class="highlight"><pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">sudo apt-get install quagga
sudo editor /etc/quagga/daemons
</code></pre></div><p>The idea is to set up an OSPF backbone, area 0, for our routers without
wrecking havoc in the big corporate intranet, which may already run OSPF
&hellip; so OSPF should only talk on the <code>gre0</code> interface, and maybe even the
LAN interfaces towards <code>C1</code> and <code>C2</code> (in case we want to expand on this
example later).  In our setup the routers use <code>wlan0</code> to connect to the
intranet.  We can use the sample configuration files to start from:</p>
<div class="highlight"><pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">sudo cp /usr/share/doc/quagga/examples/zebra.conf.sample /etc/quagga/zebra.conf
sudo cp /usr/share/doc/quagga/examples/ospfd.conf.sample /etc/quagga/ospfd.conf
</code></pre></div><p>The <code>zebra.conf</code> can be left as-is, just edit the <code>ospfd.conf</code> to look
like this:</p>
<div class="highlight"><pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">hostname ospfd
password zebra
router ospf
    passive-interface wlan0
    redistribute connected
    network 172.16.16.0/30 area 0
</code></pre></div><p>When the routers have peered, the two clients <code>C1</code> and <code>C2</code> should be
able to (unicast) ping each other.  Telnet into the OSPF daemon using
<code>telnet localhost ospfd</code> and type <code>show ip ospf neigh</code> to see all OSPF
neighbors and their status, should be <code>Full/...</code> when done exchanging
routes.  Use <code>show ip ospf route</code> to see the exchanged routes, also
inspect the kernel routing table with <code>route -n</code>.  Use <code>traceroute</code> to
confirm the traffic between clients do traverse the GRE tunnel and not
over the Intranet.</p>
<div class="highlight"><pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">root@C1:~$ traceroute 10.0.2.2
traceroute to 10.0.2.2 (10.0.2.2), <span style="color:#3677a9">30</span> hops max, <span style="color:#3677a9">60</span> byte packets
 <span style="color:#3677a9">1</span>  10.0.1.1 (10.0.1.1)  0.427 ms  0.384 ms  0.309 ms
 <span style="color:#3677a9">2</span>  172.16.16.2 (172.16.16.2)  3.135 ms  4.724 ms  5.786 ms
 <span style="color:#3677a9">3</span>  10.0.2.2 (10.0.2.2)  9.979 ms  10.777 ms  10.676 ms
</code></pre></div><p>Time for <code>pimd</code> to be started.  Just like OSPF we want <code>pimd</code> to avoid
talking on <code>wlan0</code>, so add the following to the default <code>pimd.conf</code>,
which can otherwise be left as-is:</p>
<div class="highlight"><pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">phyint wlan0 disable
</code></pre></div><p>In our case the routers have many interfaces, so we disable <em>all</em>
interfaces using the <code>-N</code> switch to <code>pimd</code> and instead only enable the
interfaces we are intrested in, the GRE tunnel and the client LAN
interface, <code>eth0</code>:</p>
<div class="highlight"><pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">phyint eth0 enable
phyint gre0 enable
</code></pre></div><p>Make sure to add the <code>phyint</code> setting to the correct place in the file.
Now start pimd on each router, use the debug mode and run in foreground
first to see what happens:</p>
<div class="highlight"><pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">root@R1:~$ pimd -N -f -d
debug level 0xffffffff (dvmrp_detail,dvmrp_prunes,dvmrp_routes,dvmrp_neighbors,dvmrp_timers,igmp_proto,igmp_timers,igmp_members,trace,timeout,packets,interfaces,kernel,cache,rsrr,pim_detail,pim_hello,pim_register,pim_join_prune,pim_bootstrap,pim_asserts,pim_cand_rp,pim_routes,pim_timers,pim_rpf)
02:01:49.616 pimd version 2.3.2 starting ...
02:01:49.617 Got <span style="color:#3677a9">262144</span> byte send buffer size in <span style="color:#3677a9">0</span> iterations
02:01:49.617 Got <span style="color:#3677a9">262144</span> byte recv buffer size in <span style="color:#3677a9">0</span> iterations
02:01:49.617 Got <span style="color:#3677a9">262144</span> byte send buffer size in <span style="color:#3677a9">0</span> iterations
02:01:49.617 Got <span style="color:#3677a9">262144</span> byte recv buffer size in <span style="color:#3677a9">0</span> iterations
02:01:49.617 Getting vifs from kernel
02:01:49.618 /etc/pimd.conf:0 - Skipping interface lo, either loopback or does not support multicast.
</code></pre></div><p>&hellip; and on <code>R2</code>:</p>
<div class="highlight"><pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">root@R2:~$ pimd -N -f -d
debug level 0xffffffff (dvmrp_detail,dvmrp_prunes,dvmrp_routes,dvmrp_neighbors,dvmrp_timers,igmp_proto,igmp_timers,igmp_members,trace,timeout,packets,interfaces,kernel,cache,rsrr,pim_detail,pim_hello,pim_register,pim_join_prune,pim_bootstrap,pim_asserts,pim_cand_rp,pim_routes,pim_timers,pim_rpf)
02:01:49.616 pimd version 2.3.2 starting ...
02:01:49.617 Got <span style="color:#3677a9">262144</span> byte send buffer size in <span style="color:#3677a9">0</span> iterations
02:01:49.617 Got <span style="color:#3677a9">262144</span> byte recv buffer size in <span style="color:#3677a9">0</span> iterations
02:01:49.617 Got <span style="color:#3677a9">262144</span> byte send buffer size in <span style="color:#3677a9">0</span> iterations
02:01:49.617 Got <span style="color:#3677a9">262144</span> byte recv buffer size in <span style="color:#3677a9">0</span> iterations
02:01:49.617 Getting vifs from kernel
02:01:49.618 /etc/pimd.conf:0 - Skipping interface lo, either loopback or does not support multicast.
</code></pre></div><p>To verify multicast routing works we start a multicast sender on <code>C1</code>
and a receiver (sink) on <code>C2</code>:</p>
<div class="highlight"><pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">root@C1:~$ mcjoin -s -t <span style="color:#3677a9">10</span>
root@C2:~$ mcjoin
.................o
</code></pre></div><p>Notice the TTL adjustment (<code>-t 10</code>) on the sender side, this is needed
because the default TTL for multicast, due to safety concerns, is 1.</p>
<p>The log on the routers should look something like this (snippet):</p>
<div class="highlight"><pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">01:44:16.750 accept_group_report(): igmp_src 10.0.2.2 ssm_src 0.0.0.0 group 225.1.2.3 report_type 34
01:44:16.750 Set delete timer for group: 225.1.2.3
01:44:16.750 Adding vif 4 for group 225.1.2.3
01:44:16.877 Received IGMP v3 Membership Report from 172.16.16.1 to 224.0.0.22
01:44:16.877 accept_membership_report(): IGMP v3 report, 40 bytes, from 172.16.16.1 to 224.0.0.22 with 4 group records.

Virtual Interface Table ======================================================
Vif  Local Address    Subnet              Thresh  Flags      Neighbors
---  ---------------  ------------------  ------  ---------  -----------------
  0  192.168.1.123    192.168.1                1  DISABLED
  1  172.17.0.1       172.17                   1  DISABLED
  2  192.168.122.1    192.168.122              1  DISABLED
  3  172.16.16.2      172.16.16/30             1  DR PIM     172.16.16.1
  4  10.0.2.1         10.0.2                   1  DR NO-NBR
  5  172.16.16.2      register_vif0            1 

 Vif  SSM Group        Sources             


Multicast Routing Table ======================================================
----------------------------------- (*,G) ------------------------------------
Source           Group            RP Address       Flags
---------------  ---------------  ---------------  ---------------------------
INADDR_ANY       225.1.2.3        10.0.2.1         WC RP
Joined   oifs: ......              
Pruned   oifs: ......              
Leaves   oifs: ....l.              
Asserted oifs: ......              
Outgoing oifs: ....o.              
Incoming     : .....I              

TIMERS:  Entry    JP    RS  Assert VIFS:  0  1  2  3  4  5
             0    45     0       0        0  0  0  0  0  0
----------------------------------- (S,G) ------------------------------------
Source           Group            RP Address       Flags
---------------  ---------------  ---------------  ---------------------------
192.168.64.2     225.1.2.3        10.0.2.1         SPT CACHE SG
Joined   oifs: ......              
Pruned   oifs: ......              
Leaves   oifs: ....l.              
Asserted oifs: ......              
Outgoing oifs: ....o.              
Incoming     : ...I..              

TIMERS:  Entry    JP    RS  Assert VIFS:  0  1  2  3  4  5
           200    55     0       0        0  0  0  0  0  0
--------------------------------- (*,*,G) ------------------------------------
Number of Groups: 1
Number of Cache MIRRORs: 1
------------------------------------------------------------------------------

Candidate Rendezvous-Point Set ===============================================
RP address       Incoming  Group Prefix        Priority  Holdtime
---------------  --------  ------------------  --------  ---------------------
10.0.2.1         5         224/4               20        70      
10.0.1.1         3         224/4               20        55      
169.254.0.1      1         232/8               1         65535   
------------------------------------------------------------------------------
Current BSR address: 10.0.2.1
</code></pre></div><p>In normal operation, i.e. without the debug flag <code>-d</code>, the routing table
and other useful PIM information can be queried from the running <code>pimd</code>
by calling it in client mode.  The following is a wrapper for sending a
<code>SIGUSR1</code> signal to the daemon and reading <code>/var/run/pimd/pimd.dump</code>:</p>
<div class="highlight"><pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">root@R2:~$ pimd -r
Virtual Interface Table ======================================================
Vif  Local Address    Subnet              Thresh  Flags      Neighbors
---  ---------------  ------------------  ------  ---------  -----------------
  0  192.168.1.123    192.168.1                1  DISABLED
  1  172.17.0.1       172.17                   1  DISABLED
  2  192.168.122.1    192.168.122              1  DISABLED
  3  172.16.16.2      172.16.16/30             1  DR PIM     172.16.16.1    
  4  10.0.2.1         10.0.2                   1  DR NO-NBR
  5  172.16.16.2      register_vif0            1 

 Vif  SSM Group        Sources             

Multicast Routing Table ======================================================
----------------------------------- (*,G) ------------------------------------
Source           Group            RP Address       Flags
---------------  ---------------  ---------------  ---------------------------
INADDR_ANY       225.1.2.3        10.0.2.1         WC RP
Joined   oifs: ......              
Pruned   oifs: ......              
Leaves   oifs: ....l.              
Asserted oifs: ......              
Outgoing oifs: ....o.              
Incoming     : .....I              

TIMERS:  Entry    JP    RS  Assert VIFS:  0  1  2  3  4  5
             0    20     0       0        0  0  0  0  0  0
----------------------------------- (S,G) ------------------------------------
Source           Group            RP Address       Flags
---------------  ---------------  ---------------  ---------------------------
10.0.1.2         225.1.2.3        10.0.2.1         SPT CACHE SG
Joined   oifs: ......              
Pruned   oifs: ......              
Leaves   oifs: ....l.              
Asserted oifs: ......              
Outgoing oifs: ....o.              
Incoming     : ...I..              

TIMERS:  Entry    JP    RS  Assert VIFS:  0  1  2  3  4  5
           185    10     0       0        0  0  0  0  0  0
--------------------------------- (*,*,G) ------------------------------------
Number of Groups: 1
Number of Cache MIRRORs: 1
------------------------------------------------------------------------------
</code></pre></div><p>You can also verify that <code>pimd</code> actually sets routes in the kernel
multicast routing table with:</p>
<div class="highlight"><pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">root@R2:~$ ip mroute
(10.0.1.2, 225.1.2.3)        Iif: gre0       Oifs: eth0
</code></pre></div><h2 id="caveats">Caveats</h2>
<p>There are quite a few caveats to watch out for with multicast routing.
Here are a few:</p>
<ol>
<li>Check the TTL of the multicast stream, must be &gt;1 to be routed.
<strong>Rule of thumb:</strong> increase with each router in topology.</li>
<li>Check the MTU of the interfaces in the path.</li>
<li>Check the reverse path, make sure you have unicast connectivity
between end nodes &ndash; PIM requires this to work!</li>
<li>Check the multicast sender&rsquo;s source IP, verify that it&rsquo;s not a
unroutable link-local (169.254) address.</li>
<li>Have the PIM routers peered?  Should list &lsquo;PIM&rsquo; and a neighbor IP
in the output of <code>pimd -r</code>.  If NO-NBR or DISABLED is shown you
have a network or <code>pimd.conf</code> problem.</li>
<li>As of this writing <code>pimd</code> is a bit sensitive to topology changes.
See <a href="https://github.com/troglobit/pimd/issues/79">issue #79</a> for
details and possible future resolution.</li>
</ol>
<p>For other questions, ideas on setting up multicast routing, see the
general <a href="https://troglobit.com/multicast-howto.html">Multicast HowTo</a>, or file a bug report
at <a href="https://github.com/troglobit">GitHub</a></p>
<p>That&rsquo;s it, good luck!</p>
  </article>
</section>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js" id="MathJax-script"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [
          ['$', '$'], ['\\(', '\\)']
        ],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
  </script>

      </div>

      
  <footer class="footer">
    <section class="container">
      
      
        ©
        
        2020
         Joachim Wiberg 
      
      
      
    </section>
  </footer>

    </main>

    
      
      <script src="https://troglobit.com/js/dark-mode.min.0213e1773e6d1c5a644f847c67a6f8abac49a3776e2976f6008038af8c5b76a1.js"></script>
    

    

    

    

    

    
  </body>

</html>
